{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.7956\n",
      "Epoch [2/200], Loss: 0.7744\n",
      "Epoch [3/200], Loss: 0.7541\n",
      "Epoch [4/200], Loss: 0.7347\n",
      "Epoch [5/200], Loss: 0.7162\n",
      "Epoch [6/200], Loss: 0.6988\n",
      "Epoch [7/200], Loss: 0.6823\n",
      "Epoch [8/200], Loss: 0.6668\n",
      "Epoch [9/200], Loss: 0.6522\n",
      "Epoch [10/200], Loss: 0.6390\n",
      "Epoch [11/200], Loss: 0.6263\n",
      "Epoch [12/200], Loss: 0.6141\n",
      "Epoch [13/200], Loss: 0.6021\n",
      "Epoch [14/200], Loss: 0.5904\n",
      "Epoch [15/200], Loss: 0.5789\n",
      "Epoch [16/200], Loss: 0.5674\n",
      "Epoch [17/200], Loss: 0.5560\n",
      "Epoch [18/200], Loss: 0.5447\n",
      "Epoch [19/200], Loss: 0.5333\n",
      "Epoch [20/200], Loss: 0.5222\n",
      "Epoch [21/200], Loss: 0.5112\n",
      "Epoch [22/200], Loss: 0.5000\n",
      "Epoch [23/200], Loss: 0.4889\n",
      "Epoch [24/200], Loss: 0.4777\n",
      "Epoch [25/200], Loss: 0.4665\n",
      "Epoch [26/200], Loss: 0.4553\n",
      "Epoch [27/200], Loss: 0.4440\n",
      "Epoch [28/200], Loss: 0.4328\n",
      "Epoch [29/200], Loss: 0.4215\n",
      "Epoch [30/200], Loss: 0.4103\n",
      "Epoch [31/200], Loss: 0.3990\n",
      "Epoch [32/200], Loss: 0.3878\n",
      "Epoch [33/200], Loss: 0.3770\n",
      "Epoch [34/200], Loss: 0.3668\n",
      "Epoch [35/200], Loss: 0.3565\n",
      "Epoch [36/200], Loss: 0.3462\n",
      "Epoch [37/200], Loss: 0.3358\n",
      "Epoch [38/200], Loss: 0.3254\n",
      "Epoch [39/200], Loss: 0.3151\n",
      "Epoch [40/200], Loss: 0.3047\n",
      "Epoch [41/200], Loss: 0.2945\n",
      "Epoch [42/200], Loss: 0.2843\n",
      "Epoch [43/200], Loss: 0.2742\n",
      "Epoch [44/200], Loss: 0.2642\n",
      "Epoch [45/200], Loss: 0.2544\n",
      "Epoch [46/200], Loss: 0.2448\n",
      "Epoch [47/200], Loss: 0.2356\n",
      "Epoch [48/200], Loss: 0.2272\n",
      "Epoch [49/200], Loss: 0.2194\n",
      "Epoch [50/200], Loss: 0.2116\n",
      "Epoch [51/200], Loss: 0.2040\n",
      "Epoch [52/200], Loss: 0.1964\n",
      "Epoch [53/200], Loss: 0.1890\n",
      "Epoch [54/200], Loss: 0.1822\n",
      "Epoch [55/200], Loss: 0.1757\n",
      "Epoch [56/200], Loss: 0.1698\n",
      "Epoch [57/200], Loss: 0.1642\n",
      "Epoch [58/200], Loss: 0.1587\n",
      "Epoch [59/200], Loss: 0.1532\n",
      "Epoch [60/200], Loss: 0.1478\n",
      "Epoch [61/200], Loss: 0.1427\n",
      "Epoch [62/200], Loss: 0.1376\n",
      "Epoch [63/200], Loss: 0.1327\n",
      "Epoch [64/200], Loss: 0.1278\n",
      "Epoch [65/200], Loss: 0.1230\n",
      "Epoch [66/200], Loss: 0.1183\n",
      "Epoch [67/200], Loss: 0.1138\n",
      "Epoch [68/200], Loss: 0.1096\n",
      "Epoch [69/200], Loss: 0.1058\n",
      "Epoch [70/200], Loss: 0.1023\n",
      "Epoch [71/200], Loss: 0.0988\n",
      "Epoch [72/200], Loss: 0.0953\n",
      "Epoch [73/200], Loss: 0.0920\n",
      "Epoch [74/200], Loss: 0.0888\n",
      "Epoch [75/200], Loss: 0.0857\n",
      "Epoch [76/200], Loss: 0.0827\n",
      "Epoch [77/200], Loss: 0.0798\n",
      "Epoch [78/200], Loss: 0.0770\n",
      "Epoch [79/200], Loss: 0.0744\n",
      "Epoch [80/200], Loss: 0.0719\n",
      "Epoch [81/200], Loss: 0.0695\n",
      "Epoch [82/200], Loss: 0.0672\n",
      "Epoch [83/200], Loss: 0.0651\n",
      "Epoch [84/200], Loss: 0.0631\n",
      "Epoch [85/200], Loss: 0.0612\n",
      "Epoch [86/200], Loss: 0.0593\n",
      "Epoch [87/200], Loss: 0.0575\n",
      "Epoch [88/200], Loss: 0.0557\n",
      "Epoch [89/200], Loss: 0.0540\n",
      "Epoch [90/200], Loss: 0.0524\n",
      "Epoch [91/200], Loss: 0.0509\n",
      "Epoch [92/200], Loss: 0.0494\n",
      "Epoch [93/200], Loss: 0.0480\n",
      "Epoch [94/200], Loss: 0.0466\n",
      "Epoch [95/200], Loss: 0.0454\n",
      "Epoch [96/200], Loss: 0.0441\n",
      "Epoch [97/200], Loss: 0.0429\n",
      "Epoch [98/200], Loss: 0.0418\n",
      "Epoch [99/200], Loss: 0.0406\n",
      "Epoch [100/200], Loss: 0.0396\n",
      "Epoch [101/200], Loss: 0.0386\n",
      "Epoch [102/200], Loss: 0.0376\n",
      "Epoch [103/200], Loss: 0.0366\n",
      "Epoch [104/200], Loss: 0.0357\n",
      "Epoch [105/200], Loss: 0.0349\n",
      "Epoch [106/200], Loss: 0.0340\n",
      "Epoch [107/200], Loss: 0.0332\n",
      "Epoch [108/200], Loss: 0.0325\n",
      "Epoch [109/200], Loss: 0.0317\n",
      "Epoch [110/200], Loss: 0.0310\n",
      "Epoch [111/200], Loss: 0.0303\n",
      "Epoch [112/200], Loss: 0.0296\n",
      "Epoch [113/200], Loss: 0.0290\n",
      "Epoch [114/200], Loss: 0.0284\n",
      "Epoch [115/200], Loss: 0.0278\n",
      "Epoch [116/200], Loss: 0.0272\n",
      "Epoch [117/200], Loss: 0.0266\n",
      "Epoch [118/200], Loss: 0.0260\n",
      "Epoch [119/200], Loss: 0.0255\n",
      "Epoch [120/200], Loss: 0.0250\n",
      "Epoch [121/200], Loss: 0.0245\n",
      "Epoch [122/200], Loss: 0.0240\n",
      "Epoch [123/200], Loss: 0.0236\n",
      "Epoch [124/200], Loss: 0.0231\n",
      "Epoch [125/200], Loss: 0.0226\n",
      "Epoch [126/200], Loss: 0.0222\n",
      "Epoch [127/200], Loss: 0.0218\n",
      "Epoch [128/200], Loss: 0.0214\n",
      "Epoch [129/200], Loss: 0.0210\n",
      "Epoch [130/200], Loss: 0.0206\n",
      "Epoch [131/200], Loss: 0.0203\n",
      "Epoch [132/200], Loss: 0.0199\n",
      "Epoch [133/200], Loss: 0.0196\n",
      "Epoch [134/200], Loss: 0.0192\n",
      "Epoch [135/200], Loss: 0.0189\n",
      "Epoch [136/200], Loss: 0.0186\n",
      "Epoch [137/200], Loss: 0.0183\n",
      "Epoch [138/200], Loss: 0.0180\n",
      "Epoch [139/200], Loss: 0.0177\n",
      "Epoch [140/200], Loss: 0.0174\n",
      "Epoch [141/200], Loss: 0.0171\n",
      "Epoch [142/200], Loss: 0.0168\n",
      "Epoch [143/200], Loss: 0.0165\n",
      "Epoch [144/200], Loss: 0.0163\n",
      "Epoch [145/200], Loss: 0.0160\n",
      "Epoch [146/200], Loss: 0.0158\n",
      "Epoch [147/200], Loss: 0.0155\n",
      "Epoch [148/200], Loss: 0.0153\n",
      "Epoch [149/200], Loss: 0.0151\n",
      "Epoch [150/200], Loss: 0.0148\n",
      "Epoch [151/200], Loss: 0.0146\n",
      "Epoch [152/200], Loss: 0.0144\n",
      "Epoch [153/200], Loss: 0.0142\n",
      "Epoch [154/200], Loss: 0.0140\n",
      "Epoch [155/200], Loss: 0.0138\n",
      "Epoch [156/200], Loss: 0.0136\n",
      "Epoch [157/200], Loss: 0.0134\n",
      "Epoch [158/200], Loss: 0.0132\n",
      "Epoch [159/200], Loss: 0.0130\n",
      "Epoch [160/200], Loss: 0.0129\n",
      "Epoch [161/200], Loss: 0.0127\n",
      "Epoch [162/200], Loss: 0.0125\n",
      "Epoch [163/200], Loss: 0.0124\n",
      "Epoch [164/200], Loss: 0.0122\n",
      "Epoch [165/200], Loss: 0.0120\n",
      "Epoch [166/200], Loss: 0.0119\n",
      "Epoch [167/200], Loss: 0.0117\n",
      "Epoch [168/200], Loss: 0.0116\n",
      "Epoch [169/200], Loss: 0.0114\n",
      "Epoch [170/200], Loss: 0.0113\n",
      "Epoch [171/200], Loss: 0.0111\n",
      "Epoch [172/200], Loss: 0.0110\n",
      "Epoch [173/200], Loss: 0.0109\n",
      "Epoch [174/200], Loss: 0.0107\n",
      "Epoch [175/200], Loss: 0.0106\n",
      "Epoch [176/200], Loss: 0.0105\n",
      "Epoch [177/200], Loss: 0.0103\n",
      "Epoch [178/200], Loss: 0.0102\n",
      "Epoch [179/200], Loss: 0.0101\n",
      "Epoch [180/200], Loss: 0.0100\n",
      "Epoch [181/200], Loss: 0.0099\n",
      "Epoch [182/200], Loss: 0.0097\n",
      "Epoch [183/200], Loss: 0.0096\n",
      "Epoch [184/200], Loss: 0.0095\n",
      "Epoch [185/200], Loss: 0.0094\n",
      "Epoch [186/200], Loss: 0.0093\n",
      "Epoch [187/200], Loss: 0.0092\n",
      "Epoch [188/200], Loss: 0.0091\n",
      "Epoch [189/200], Loss: 0.0090\n",
      "Epoch [190/200], Loss: 0.0089\n",
      "Epoch [191/200], Loss: 0.0088\n",
      "Epoch [192/200], Loss: 0.0087\n",
      "Epoch [193/200], Loss: 0.0086\n",
      "Epoch [194/200], Loss: 0.0085\n",
      "Epoch [195/200], Loss: 0.0084\n",
      "Epoch [196/200], Loss: 0.0083\n",
      "Epoch [197/200], Loss: 0.0082\n",
      "Epoch [198/200], Loss: 0.0081\n",
      "Epoch [199/200], Loss: 0.0081\n",
      "Epoch [200/200], Loss: 0.0080\n",
      "Decoded bits: tensor([0, 1, 0, 1])\n",
      "True bits:    tensor([0, 1, 0, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 定义简单的两层神经网络\n",
    "class RZDecoderNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RZDecoderNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# 从文件中加载数据\n",
    "def load_data_from_file(file_path, bit_length):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = [float(x) for x in line.strip().split()]\n",
    "            inputs.append(values[:bit_length])  # 前 bit_length 个值为输入信号\n",
    "            outputs.append(values[bit_length])  # 最后一个值为输出标签\n",
    "\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    outputs = torch.tensor(outputs, dtype=torch.float32).unsqueeze(1)  # 添加维度以匹配网络输出\n",
    "    return inputs, outputs\n",
    "\n",
    "# 训练 RZ 解码网络\n",
    "def train_rz_decoder(model, train_loader, num_epochs=20, learning_rate=0.01):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 解码 RZ 信号\n",
    "def rz_decode_with_nn(model, signal, bit_length):\n",
    "    model.eval()\n",
    "    decoded_bits = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(signal)):\n",
    "            bit_window = signal[i].unsqueeze(0)\n",
    "            output = model(bit_window)\n",
    "            decoded_bits.append(1 if output >= 0.5 else 0)\n",
    "    return torch.tensor(decoded_bits)\n",
    "\n",
    "# 参数\n",
    "bit_length = 1  # 每个比特的长度\n",
    "\n",
    "# 从文件加载训练和测试数据\n",
    "train_file_path = r'E:\\gitprogram\\data\\testdata\\train.txt'\n",
    "test_file_path = r'E:\\gitprogram\\data\\testdata\\test.txt'\n",
    "\n",
    "# 加载数据\n",
    "train_inputs, train_labels = load_data_from_file(train_file_path, bit_length)\n",
    "test_inputs, test_labels = load_data_from_file(test_file_path, bit_length)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# 初始化网络\n",
    "input_size = bit_length\n",
    "model = RZDecoderNet(input_size)\n",
    "\n",
    "# 训练网络\n",
    "train_rz_decoder(model, train_loader, num_epochs=200, learning_rate=0.01)\n",
    "\n",
    "# 使用测试数据进行解码\n",
    "decoded_bits = rz_decode_with_nn(model, test_inputs, bit_length)\n",
    "print(\"Decoded bits:\", decoded_bits)\n",
    "print(\"True bits:   \", test_labels.squeeze().int())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
